{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "import html\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df_comments = pd.read_pickle('./pickle_dataframes/comments_typecasted.pkl')\n",
    "df_posts = pd.read_pickle('./pickle_dataframes/posts_typecasted.pkl')\n",
    "\n",
    "df_postlinks = pd.read_pickle('./pickle_dataframes/post_links_typecasted.pkl')\n",
    "df_tags = pd.read_pickle('./pickle_dataframes/tags_typecasted.pkl')\n",
    "df_users = pd.read_pickle('./pickle_dataframes/users_typecasted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>UserId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>47428</td>\n",
       "      <td>3</td>\n",
       "      <td>One of the things that make a url user-friendl...</td>\n",
       "      <td>2008-09-06 13:51:47.843</td>\n",
       "      <td>4642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>47481</td>\n",
       "      <td>0</td>\n",
       "      <td>I agree, both CodeRush and RefactorPro are vis...</td>\n",
       "      <td>2008-09-06 14:15:46.897</td>\n",
       "      <td>4642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>47373</td>\n",
       "      <td>0</td>\n",
       "      <td>Just wanted to mention that this is an excelle...</td>\n",
       "      <td>2008-09-06 14:30:40.217</td>\n",
       "      <td>2495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>47497</td>\n",
       "      <td>1</td>\n",
       "      <td>Indeed, the only way to do this is get the ser...</td>\n",
       "      <td>2008-09-06 14:42:35.303</td>\n",
       "      <td>4642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>1517</td>\n",
       "      <td>0</td>\n",
       "      <td>In the interests of tact, this is the kind of ...</td>\n",
       "      <td>2008-09-06 15:44:39.477</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89336446</th>\n",
       "      <td>135796215</td>\n",
       "      <td>77031085</td>\n",
       "      <td>0</td>\n",
       "      <td>`sub(' R-HSA.*', '', a)`</td>\n",
       "      <td>2023-09-03 06:01:39.930</td>\n",
       "      <td>3962914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89336448</th>\n",
       "      <td>135796217</td>\n",
       "      <td>77031076</td>\n",
       "      <td>0</td>\n",
       "      <td>At the risk of stating the obvious, are you us...</td>\n",
       "      <td>2023-09-03 06:02:27.873</td>\n",
       "      <td>2288659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89336450</th>\n",
       "      <td>135796219</td>\n",
       "      <td>77030109</td>\n",
       "      <td>0</td>\n",
       "      <td>no problem. could you also upvote and accept m...</td>\n",
       "      <td>2023-09-03 06:02:45.790</td>\n",
       "      <td>22240478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89336451</th>\n",
       "      <td>135796220</td>\n",
       "      <td>77031063</td>\n",
       "      <td>0</td>\n",
       "      <td>remove await and async from getdistricts</td>\n",
       "      <td>2023-09-03 06:03:14.907</td>\n",
       "      <td>22196971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89336453</th>\n",
       "      <td>135796222</td>\n",
       "      <td>12138267</td>\n",
       "      <td>0</td>\n",
       "      <td>The first three words are right. The rest is w...</td>\n",
       "      <td>2023-09-03 06:04:05.750</td>\n",
       "      <td>1290731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47612714 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id    PostId  Score  \\\n",
       "1                12     47428      3   \n",
       "2                14     47481      0   \n",
       "3                15     47373      0   \n",
       "4                16     47497      1   \n",
       "7                20      1517      0   \n",
       "...             ...       ...    ...   \n",
       "89336446  135796215  77031085      0   \n",
       "89336448  135796217  77031076      0   \n",
       "89336450  135796219  77030109      0   \n",
       "89336451  135796220  77031063      0   \n",
       "89336453  135796222  12138267      0   \n",
       "\n",
       "                                                       Text  \\\n",
       "1         One of the things that make a url user-friendl...   \n",
       "2         I agree, both CodeRush and RefactorPro are vis...   \n",
       "3         Just wanted to mention that this is an excelle...   \n",
       "4         Indeed, the only way to do this is get the ser...   \n",
       "7         In the interests of tact, this is the kind of ...   \n",
       "...                                                     ...   \n",
       "89336446                           `sub(' R-HSA.*', '', a)`   \n",
       "89336448  At the risk of stating the obvious, are you us...   \n",
       "89336450  no problem. could you also upvote and accept m...   \n",
       "89336451           remove await and async from getdistricts   \n",
       "89336453  The first three words are right. The rest is w...   \n",
       "\n",
       "                    CreationDate    UserId  \n",
       "1        2008-09-06 13:51:47.843      4642  \n",
       "2        2008-09-06 14:15:46.897      4642  \n",
       "3        2008-09-06 14:30:40.217      2495  \n",
       "4        2008-09-06 14:42:35.303      4642  \n",
       "7        2008-09-06 15:44:39.477       199  \n",
       "...                          ...       ...  \n",
       "89336446 2023-09-03 06:01:39.930   3962914  \n",
       "89336448 2023-09-03 06:02:27.873   2288659  \n",
       "89336450 2023-09-03 06:02:45.790  22240478  \n",
       "89336451 2023-09-03 06:03:14.907  22196971  \n",
       "89336453 2023-09-03 06:04:05.750   1290731  \n",
       "\n",
       "[47612714 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = df_posts[df_posts['PostTypeId'] == 1]\n",
    "sub_posts = df_posts[df_posts['PostTypeId'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic model: only posts from active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove entries with -1 in UserId and OwnerUserId columns\n",
    "df_comments = df_comments[df_comments['UserId'] != -1]\n",
    "df_posts = df_posts[df_posts['OwnerUserId'] != -1]\n",
    "\n",
    "# Calculate post and comment counts for each user\n",
    "user_posts_count = df_posts.groupby('OwnerUserId')['OwnerUserId'].size().rename('PostCount')\n",
    "user_comments_count = df_comments.groupby('UserId')['UserId'].size().rename('CommentCount')\n",
    "\n",
    "# Merge counts with user data and fill missing values\n",
    "user_data = (df_users\n",
    "             .merge(user_posts_count, left_on='Id', right_index=True, how='left')\n",
    "             .merge(user_comments_count, left_on='Id', right_index=True, how='left')\n",
    "             .fillna({'PostCount': 0, 'CommentCount': 0}))\n",
    "\n",
    "# Add a column for total activity and filter for active users\n",
    "active_users = user_data.assign(TotalActivity=lambda x: x['PostCount'] + x['CommentCount'])\n",
    "active_users = active_users[active_users['TotalActivity'] > 20]\n",
    "\n",
    "# Set of active user IDs\n",
    "active_user_ids = set(active_users['Id'])\n",
    "\n",
    "# Filter dataframes for active user activity\n",
    "filtered_questions_df = questions_df[questions_df['OwnerUserId'].isin(active_user_ids)]\n",
    "filtered_sub_posts = sub_posts[(sub_posts['ParentId'].isin(active_user_ids)) |\n",
    "                                (sub_posts['Id'].isin(active_user_ids))]\n",
    "\n",
    "# Combine filters for comments related to active users\n",
    "active_user_post_ids = set(df_posts[df_posts['OwnerUserId'].isin(active_user_ids)]['Id'])\n",
    "filtered_comments = df_comments[(df_comments['UserId'].isin(active_user_ids)) | \n",
    "                                (df_comments['PostId'].isin(active_user_post_ids))].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224028\n",
      "139902 \n",
      "\n",
      "505260\n",
      "13342 \n",
      "\n",
      "1054752\n",
      "998796 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(questions_df.size)\n",
    "print(filtered_questions_df.size, '\\n')\n",
    "\n",
    "print(sub_posts.size)\n",
    "print(filtered_sub_posts.size, '\\n')\n",
    "\n",
    "print(df_comments.size)\n",
    "print(filtered_comments.size, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify preprocess_text function\n",
    "def preprocess_text(text, remove_stopwords=False, use_lemmatize=True, use_stemmer=False):\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower())\n",
    "\n",
    "    words = text.split()\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "    if use_lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    elif use_stemmer:  # Apply stemming only if use_stemmer is True\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB Timeeee\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define apply_lda_and_log function with run_name parameter\n",
    "def apply_topic_modeling_and_log(df, remove_stopwords, use_lemmatize, use_stemmer, tags_weighting, run_name, ngram_range=(1, 1), max_features=1000):\n",
    "    # Start a new WandB run with the specified name\n",
    "    wandb.init(project=\"stackexchange_politics\", entity=\"s223730\", name=run_name)\n",
    "\n",
    "    # Initialize dictionaries to store topic distributions\n",
    "    lda_distributions = {}\n",
    "    nmf_distributions = {}\n",
    "\n",
    "    # Preprocess Title, Body, and Tags\n",
    "    df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Body'] = df['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "    df['Tags'] = df['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize, use_stemmer))\n",
    "\n",
    "\n",
    "    # Combine Title, Body, and Tags with specified weight for Tags\n",
    "    # We Keep the original order (title, body, tags) as it reflects the natural flow of information\n",
    "    df['CombinedText'] = df['Title'] + ' ' + df['Body'] + ' ' + (df['Tags'] * tags_weighting)\n",
    "\n",
    "    # Create a Dictionary and Corpus needed for Topic Modeling\n",
    "    words = [doc.split() for doc in df['CombinedText']]\n",
    "    id2word = corpora.Dictionary(words)\n",
    "    corpus = [id2word.doc2bow(text) for text in words]\n",
    "\n",
    "    # Apply TF-IDF with the specified max_features\n",
    "    # ngram_range=(1, 2) for bi-grams, (1, 3) for tri-grams, and (2, 2) for only bi-grams\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['CombinedText'])\n",
    "\n",
    "    # Apply LDA and NMF for different numbers of topics\n",
    "    # Prepare a structured dictionary to store results with n_topics as part of the key\n",
    "    all_topics_results = {}\n",
    "    for n_topics in [5, 10, 15, 20]:\n",
    "        \n",
    "        # LDA\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "        lda.fit(tfidf_matrix)\n",
    "\n",
    "        # Extract Topic Distributions for LDA\n",
    "        lda_topic_distributions = lda.transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize LDA Topic Distributions\n",
    "        lda_normalized = np.array(lda_topic_distributions) / np.sum(lda_topic_distributions, axis=1)[:, None]\n",
    "\n",
    "        # Calculate Coherence Score\n",
    "        lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=0)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_gensim, texts=words, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        # Calculate LDA Perplexity\n",
    "        lda_perplexity = lda.perplexity(tfidf_matrix)\n",
    "\n",
    "        # Log Coherence and Perplexity Score\n",
    "        wandb.log({\"coherence_score\": coherence_lda, \"perplexity_score\": lda.perplexity(tfidf_matrix)})\n",
    "        \n",
    "        # Extract and log the top words for each topic as a table\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        top_words_data = []\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        # Create a WandB Table with top words data\n",
    "        columns = [\"Topic\"] + [f\"Word {i+1}\" for i in range(10)]\n",
    "        top_words_table = wandb.Table(data=top_words_data, columns=columns)\n",
    "        \n",
    "        # Log the table to WandB\n",
    "        wandb.log({f\"n_topics_{n_topics}_cleaned_{str(remove_stopwords)}_lemmatize_{str(use_lemmatize)}_weight_{tags_weighting}\": top_words_table})\n",
    "\n",
    "        # NMF\n",
    "        nmf_model = NMF(n_components=n_topics, random_state=0)\n",
    "        nmf_W = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize NMF Topic Distributions (nmf_W is already the topic distribution matrix)\n",
    "        nmf_normalized = np.array(nmf_W) / np.sum(nmf_W, axis=1)[:, None]\n",
    "\n",
    "        nmf_H = nmf_model.components_\n",
    "\n",
    "        # Calculate NMF Reconstruction Error\n",
    "        nmf_reconstruction_error = np.linalg.norm(tfidf_matrix - nmf_W.dot(nmf_H))\n",
    "\n",
    "        # Log the top words for each topic for NMF\n",
    "        nmf_top_words_data = []\n",
    "        for topic_idx, topic in enumerate(nmf_H):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            nmf_top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        nmf_top_words_table = wandb.Table(data=nmf_top_words_data, columns=columns)\n",
    "        wandb.log({f\"nmf_n_topics_{n_topics}\": nmf_top_words_table})\n",
    "\n",
    "        # Store the results including perplexity and reconstruction error\n",
    "        all_topics_results[f\"{run_name}_n_topics_{n_topics}\"] = {\n",
    "            'lda_normalized': lda_normalized,\n",
    "            'nmf_normalized': nmf_normalized,\n",
    "            'lda_coherence': coherence_lda,\n",
    "            'lda_perplexity': lda_perplexity,\n",
    "            'nmf_reconstruction_error': nmf_reconstruction_error,\n",
    "            'lda_top_words': top_words_data,\n",
    "            'nmf_top_words': nmf_top_words_data\n",
    "        }\n",
    "        \n",
    "    # Close WandB run\n",
    "    wandb.finish()\n",
    "\n",
    "    # Return the topic distributions\n",
    "    return all_topics_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running different LDA configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameter ranges\n",
    "remove_stopwords = True\n",
    "use_lemmatize_options = [False, True]\n",
    "use_stemmer_options = [False, True]\n",
    "tags_weighting_options = [1, 2, 5]\n",
    "ngram_range_options = [(1, 1), (1, 2), (1, 3)]\n",
    "max_features_options = [1000, 2000]\n",
    "\n",
    "# Store the results for each n_topics uniquely\n",
    "all_results = {}\n",
    "\n",
    "# Iterate over the combinations of other options\n",
    "for use_lemmatize, use_stemmer, tags_weighting, ngram_range, max_features in itertools.product(use_lemmatize_options, use_stemmer_options, tags_weighting_options, ngram_range_options, max_features_options):\n",
    "    \n",
    "    # Skip the iteration if both lemmatize and stemmer are set to True\n",
    "    if use_lemmatize and use_stemmer:\n",
    "        continue\n",
    "    \n",
    "    # Construct a unique run name for this combination\n",
    "    run_name = f\"Run_remove_{remove_stopwords}_lemmatize_{use_lemmatize}_stemmer_{use_stemmer}_weight_{tags_weighting}_ngram_{ngram_range}_maxfeat_{max_features}\"\n",
    "\n",
    "    # Run the function and get the results\n",
    "    topics_results = apply_topic_modeling_and_log(\n",
    "        questions_df, \n",
    "        remove_stopwords, \n",
    "        use_lemmatize, \n",
    "        use_stemmer, \n",
    "        tags_weighting, \n",
    "        run_name, \n",
    "        ngram_range, \n",
    "        max_features\n",
    "    )\n",
    "\n",
    "    # Update all_results to include these structured results\n",
    "    all_results.update(topics_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a file or handle them as needed\n",
    "# For example, saving to a pickle file\n",
    "import pickle\n",
    "with open('topic_modeling_results_n_topics.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for Improvements\n",
    "- Adjust StopWords?\n",
    "- **Hyperparameter Tuning**: Tune the parameters of the LDA model,\n",
    "    - learning decay\n",
    "    - batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Should only Post-Level have topic assigned to them?\n",
    "    - Then Sub-Posts are assigned the same topic as Post\n",
    "    - Comments are assigned the same topic as Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df.to_pickle('questions_cleaned_text.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Apply Sentiment Analysis on:\n",
    "- Post Level\n",
    "- Sub Post Level\n",
    "- Comment Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-Post-Topic Matrix**: \n",
    "- Create a matrix where rows represent users and columns represent topics. \n",
    "- Each cell contains the count of posts/comments a user has made in a particular topic.\n",
    "    - Post Level: where `PostTypeId` == 1 AND `ParentId` == -1\n",
    "    - Sub Post Level: where `PostTypeId` == 1 AND `ParentId` != -1\n",
    "    - Comment Level: where `PostTypeId` == 2\n",
    "- **Include Post Statistics**\n",
    "    - AcceptedAnswerId\n",
    "    - Score\n",
    "    - ViewCount\n",
    "    - AnswerCount\n",
    "    - CommentCount\n",
    "- **Include Comment Statistics**\n",
    "    - Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering Algorithms**\n",
    "- K-Means: Use the user-topic matrix to cluster users. Determine the optimal number of clusters (communities) using the Elbow method or Silhouette score.\n",
    "\n",
    "- Hierarchical Clustering: Useful for understanding the data structure and forming hierarchical communities. Dendrograms can visualize the community structure.\n",
    "\n",
    "- DBSCAN: Good for datasets with noise and clusters of varying densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Market Basket Analysis**\n",
    "- Association Rules and Apriori Algorithm: \n",
    "    - Treat each user's set of topics as a 'basket'. \n",
    "    - Identify strong rules where the presence of one topic implies the presence of another in a user's posts\n",
    "    - This can highlight topic-based communities.\n",
    "- Frequent Itemsets: \n",
    "    - Identify sets of topics that frequently occur together in users' posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locality Sensitive Hashing (LSH)**\n",
    "- LSH for Dimension Reduction: \n",
    "    - If the user-topic matrix is very sparse and high-dimensional, LSH can reduce dimensions while preserving the similarity structure. This can make subsequent clustering more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Techniques**\n",
    "- PCY Algorithm: If you're dealing with very large data, this algorithm efficiently finds frequent itemsets, useful in subsequent association rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Davies-Bouldin Index**: Evaluate the quality of clusters. \n",
    "- Lower Davies-Bouldin index values signify better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
