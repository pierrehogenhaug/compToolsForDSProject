{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "import html\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df_c = pd.read_pickle('./pickle_dataframes/comments_typecasted.pkl')\n",
    "df_p = pd.read_pickle('./pickle_dataframes/posts_typecasted.pkl')\n",
    "\n",
    "df_pl = pd.read_pickle('./pickle_dataframes/post_links_typecasted.pkl')\n",
    "df_t = pd.read_pickle('./pickle_dataframes/tags_typecasted.pkl')\n",
    "df_u = pd.read_pickle('./pickle_dataframes/users_typecasted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = df_c.copy()\n",
    "df_posts = df_p.copy()\n",
    "#df_post_links = df_p.copy()\n",
    "#df_tags = df_t.copy()\n",
    "df_users = df_u.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-03 09:33:40.880000\n"
     ]
    }
   ],
   "source": [
    "max_date_comments = df_comments[\"CreationDate\"].max()\n",
    "max_date_posts = df_posts[\"CreationDate\"].max()\n",
    "\n",
    "# Use the latest date as the reference for the n-year filter\n",
    "max_date = max(max_date_comments, max_date_posts)\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic model: only posts from active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove entries with -1 in UserId and OwnerUserId columns\n",
    "df_comments = df_comments[df_comments['UserId'] != -1]\n",
    "df_posts = df_posts[df_posts['OwnerUserId'] != -1]\n",
    "\n",
    "# Selecting only questions from posts\n",
    "questions_df = df_posts[df_posts['PostTypeId'] == 1]\n",
    "answers_df = df_posts[df_posts['PostTypeId'] == 2]\n",
    "\n",
    "#print(\"Number of comments \", df_comments.shape[0])\n",
    "#print(\"Number of posts \", df_posts.shape[0])\n",
    "#print(\"Number of questions \", questions_df.shape[0])\n",
    "#print(\"Number of answers \", answers_df.shape[0])\n",
    "\n",
    "# Count unique users involved in questions and comments\n",
    "unique_user_count = len(pd.concat([questions_df[\"OwnerUserId\"], answers_df[\"OwnerUserId\"], df_comments[\"UserId\"]]).unique())\n",
    "print(\"Number of Unique Users: \", unique_user_count)\n",
    "\n",
    "# Calculate post and comment counts for each user\n",
    "user_posts_count = df_posts.groupby('OwnerUserId').size().rename('PostCount')\n",
    "user_comments_count = df_comments.groupby('UserId').size().rename('CommentCount')\n",
    "\n",
    "# Merge counts with user data and fill missing values with 0\n",
    "user_data = df_users.merge(user_posts_count, left_on='Id', right_index=True, how='left')\n",
    "user_data = user_data.merge(user_comments_count, left_on='Id', right_index=True, how='left')\n",
    "user_data.fillna({'PostCount': 0, 'CommentCount': 0}, inplace=True)\n",
    "\n",
    "# Add a column for total activity and filter for active users\n",
    "user_data['TotalActivity'] = user_data['PostCount'] + user_data['CommentCount']\n",
    "\n",
    "# Get sets of active user IDs before Activity Threshold\n",
    "print(\"Users before Activity Threshold: \" , len(set(user_data['Id'])))\n",
    "\n",
    "# Get sets of active user IDs after Activity Threshold\n",
    "active_users = user_data[user_data['TotalActivity'] > 200]\n",
    "active_user_ids = set(active_users['Id'])\n",
    "print(\"Users after Activity Threshold: \" ,len(active_user_ids))\n",
    "\n",
    "# Filter questions and comments for active user activity\n",
    "filtered_questions_df = questions_df[questions_df['OwnerUserId'].isin(active_user_ids)]\n",
    "active_user_post_ids = set(df_posts[df_posts['OwnerUserId'].isin(active_user_ids)]['Id'])\n",
    "filtered_comments = df_comments[(df_comments['UserId'].isin(active_user_ids)) | \n",
    "                                (df_comments['PostId'].isin(active_user_post_ids))].drop_duplicates()\n",
    "\n",
    "# Count unique users in filtered questions and comments\n",
    "unique_active_user_count = len(pd.concat([filtered_questions_df[\"OwnerUserId\"], filtered_comments[\"UserId\"]]).unique())\n",
    "\n",
    "#print(\"Number of comments \", filtered_comments.shape[0])\n",
    "#print(\"Number of posts \", active_user_post_ids.shape[0])\n",
    "#print(\"Number of Unique Users: \", unique_active_user_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on filtering results\n",
    "Before any modifications\n",
    "- Comments: 47.612.714\n",
    "- Posts: 2.622.597\n",
    "- Users: 1.116.788\n",
    "\n",
    "ALL Creation Date, (Posts + Comments) > 50\n",
    "- Comments: 41.851.416\n",
    "- Posts: 881.872\n",
    "- Users: 272.634\n",
    "\n",
    "Creation Date Last 10 years, (Posts + Comments) > 50\n",
    "- Comments: 32.035.440\n",
    "- Posts: 719.261\n",
    "- Users: 235.129\n",
    "\n",
    "Creation Date Last 5 years, (Posts + Comments) > 50\n",
    "- Comments: 12.793.350\n",
    "- Posts: 304.070\n",
    "- Users: 120.814\n",
    "\n",
    "Creation Date Last 10 years, (Posts + Comments) > 100\n",
    "- Comments: 32.035.440\n",
    "- Posts: 719.261\n",
    "- Users: 1073398"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify preprocess_text function\n",
    "def preprocess_text(text, remove_stopwords=False, use_lemmatize=True):\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower())\n",
    "\n",
    "    words = text.split()\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "    if use_lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WandB Timeeee\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define apply_lda_and_log function with run_name parameter\n",
    "def apply_topic_modeling_and_log(df, remove_stopwords, use_lemmatize, tags_weighting, run_name, ngram_range=(1, 1), max_features=2000):\n",
    "    # Start a new WandB run with the specified name\n",
    "    wandb.init(project=\"stackexchange_politics\", entity=\"s223730\", name=run_name)\n",
    "\n",
    "    # Initialize dictionaries to store topic distributions\n",
    "    lda_distributions = {}\n",
    "    nmf_distributions = {}\n",
    "\n",
    "    # Preprocess Title, Body, and Tags\n",
    "    df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize ))\n",
    "    df['Body'] = df['Body'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize ))\n",
    "    df['Tags'] = df['Tags'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize))\n",
    "\n",
    "\n",
    "    # Combine Title, Body, and Tags with specified weight for Tags\n",
    "    # We Keep the original order (title, body, tags) as it reflects the natural flow of information\n",
    "    df['CombinedText'] = df['Title'] + ' ' + df['Body'] + ' ' + (df['Tags'] * tags_weighting)\n",
    "\n",
    "    # Create a Dictionary and Corpus needed for Topic Modeling\n",
    "    words = [doc.split() for doc in df['CombinedText']]\n",
    "    id2word = corpora.Dictionary(words)\n",
    "    corpus = [id2word.doc2bow(text) for text in words]\n",
    "\n",
    "    # Apply TF-IDF with the specified max_features\n",
    "    # ngram_range=(1, 2) for bi-grams, (1, 3) for tri-grams, and (2, 2) for only bi-grams\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['CombinedText'])\n",
    "\n",
    "    # Apply LDA and NMF for different numbers of topics\n",
    "    # Prepare a structured dictionary to store results with n_topics as part of the key\n",
    "    all_topics_results = {}\n",
    "    for n_topics in [5, 10, 15, 20]:\n",
    "        \n",
    "        # LDA\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "        lda.fit(tfidf_matrix)\n",
    "\n",
    "        # Extract Topic Distributions for LDA\n",
    "        lda_topic_distributions = lda.transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize LDA Topic Distributions\n",
    "        lda_normalized = np.array(lda_topic_distributions) / np.sum(lda_topic_distributions, axis=1)[:, None]\n",
    "\n",
    "        # Calculate Coherence Score\n",
    "        lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=0)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_gensim, texts=words, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        # Calculate LDA Perplexity\n",
    "        lda_perplexity = lda.perplexity(tfidf_matrix)\n",
    "\n",
    "        # Log Coherence and Perplexity Score\n",
    "        wandb.log({\"coherence_score\": coherence_lda, \"perplexity_score\": lda.perplexity(tfidf_matrix)})\n",
    "        \n",
    "        # Extract and log the top words for each topic as a table\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        top_words_data = []\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        # Create a WandB Table with top words data\n",
    "        columns = [\"Topic\"] + [f\"Word {i+1}\" for i in range(10)]\n",
    "        top_words_table = wandb.Table(data=top_words_data, columns=columns)\n",
    "        \n",
    "        # Log the table to WandB\n",
    "        wandb.log({f\"n_topics_{n_topics}_cleaned_{str(remove_stopwords)}_lemmatize_{str(use_lemmatize)}_weight_{tags_weighting}\": top_words_table})\n",
    "\n",
    "        # NMF\n",
    "        nmf_model = NMF(n_components=n_topics, random_state=0)\n",
    "        nmf_W = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Normalize NMF Topic Distributions (nmf_W is already the topic distribution matrix)\n",
    "        nmf_normalized = np.array(nmf_W) / np.sum(nmf_W, axis=1)[:, None]\n",
    "\n",
    "        nmf_H = nmf_model.components_\n",
    "\n",
    "        # Calculate NMF Reconstruction Error\n",
    "        nmf_reconstruction_error = np.linalg.norm(tfidf_matrix - nmf_W.dot(nmf_H))\n",
    "\n",
    "        # Log the top words for each topic for NMF\n",
    "        nmf_top_words_data = []\n",
    "        for topic_idx, topic in enumerate(nmf_H):\n",
    "            top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "            nmf_top_words_data.append([f\"Topic {topic_idx}\"] + top_words)\n",
    "\n",
    "        nmf_top_words_table = wandb.Table(data=nmf_top_words_data, columns=columns)\n",
    "        wandb.log({f\"nmf_n_topics_{n_topics}\": nmf_top_words_table})\n",
    "\n",
    "        # Store the results including perplexity and reconstruction error\n",
    "        all_topics_results[f\"{run_name}_n_topics_{n_topics}\"] = {\n",
    "            'lda_normalized': lda_normalized,\n",
    "            'nmf_normalized': nmf_normalized,\n",
    "            'lda_coherence': coherence_lda,\n",
    "            'lda_perplexity': lda_perplexity,\n",
    "            'nmf_reconstruction_error': nmf_reconstruction_error,\n",
    "            'lda_top_words': top_words_data,\n",
    "            'nmf_top_words': nmf_top_words_data\n",
    "        }\n",
    "        \n",
    "    # Close WandB run\n",
    "    wandb.finish()\n",
    "\n",
    "    # Return the topic distributions\n",
    "    return all_topics_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running different LDA configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:0sltml5a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe85e549b3b48668cb219915a9242ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Run_remove_True_lemmatize_False_weight_1_ngram_1_1_maxfeat_2000</strong> at: <a href='https://wandb.ai/s223730/stackexchange_politics/runs/0sltml5a' target=\"_blank\">https://wandb.ai/s223730/stackexchange_politics/runs/0sltml5a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231124_183335-0sltml5a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:0sltml5a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb3d9eb254141c4bde2b99e7587c57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167596755290611, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/wandb/run-20231124_183359-txwnptp8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/s223730/stackexchange_politics/runs/txwnptp8' target=\"_blank\">Run_remove_True_lemmatize_False_weight_1_ngram_1_1_maxfeat_2000</a></strong> to <a href='https://wandb.ai/s223730/stackexchange_politics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/s223730/stackexchange_politics' target=\"_blank\">https://wandb.ai/s223730/stackexchange_politics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/s223730/stackexchange_politics/runs/txwnptp8' target=\"_blank\">https://wandb.ai/s223730/stackexchange_politics/runs/txwnptp8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t0/tn_njz2x7w1_5n3k_13tz4740000gn/T/ipykernel_56628/2558294927.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Title'] = df['Title'].apply(lambda x: preprocess_text(x, remove_stopwords, use_lemmatize ))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m run_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRun_remove_\u001b[39m\u001b[39m{\u001b[39;00mremove_stopwords\u001b[39m}\u001b[39;00m\u001b[39m_lemmatize_\u001b[39m\u001b[39m{\u001b[39;00muse_lemmatize\u001b[39m}\u001b[39;00m\u001b[39m_weight_\u001b[39m\u001b[39m{\u001b[39;00mtags_weighting\u001b[39m}\u001b[39;00m\u001b[39m_ngram_\u001b[39m\u001b[39m{\u001b[39;00mngram_range_str\u001b[39m}\u001b[39;00m\u001b[39m_maxfeat_2000\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Run the function and get the results\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m topics_results \u001b[39m=\u001b[39m apply_topic_modeling_and_log(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     questions_df, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     remove_stopwords, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     use_lemmatize, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     tags_weighting, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     run_name, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     ngram_range, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     max_features\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Update all_results to include these structured results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m all_results\u001b[39m.\u001b[39mupdate(topics_results)\n",
      "\u001b[1;32m/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Preprocess Title, Body, and Tags\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: preprocess_text(x, remove_stopwords, use_lemmatize ))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mBody\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mBody\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: preprocess_text(x, remove_stopwords, use_lemmatize ))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mTags\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mTags\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: preprocess_text(x, remove_stopwords, use_lemmatize))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Combine Title, Body, and Tags with specified weight for Tags\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# We Keep the original order (title, body, tags) as it reflects the natural flow of information\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1157\u001b[0m             values,\n\u001b[1;32m   1158\u001b[0m             f,\n\u001b[1;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Preprocess Title, Body, and Tags\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: preprocess_text(x, remove_stopwords, use_lemmatize ))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mBody\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mBody\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: preprocess_text(x, remove_stopwords, use_lemmatize ))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mTags\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mTags\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: preprocess_text(x, remove_stopwords, use_lemmatize))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Combine Title, Body, and Tags with specified weight for Tags\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# We Keep the original order (title, body, tags) as it reflects the natural flow of information\u001b[39;00m\n",
      "\u001b[1;32m/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m words \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39msplit()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m remove_stopwords:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m words \u001b[39mif\u001b[39;49;00m word \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m use_lemmatize:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n",
      "\u001b[1;32m/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m words \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39msplit()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m remove_stopwords:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m use_lemmatize:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phog/Desktop/cleverThings/socialGraphs23/stackoverflow_dump/compToolsForDSProject/topic_extraction_WandB_3.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/nltk/corpus/reader/wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m     20\u001b[0m         line\n\u001b[0;32m---> 21\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[1;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[1;32m     23\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/nltk/corpus/reader/api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    216\u001b[0m contents \u001b[39m=\u001b[39m []\n\u001b[1;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids:\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(f) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m    219\u001b[0m         contents\u001b[39m.\u001b[39mappend(fp\u001b[39m.\u001b[39mread())\n\u001b[1;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/nltk/corpus/reader/api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding(file)\n\u001b[0;32m--> 231\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root\u001b[39m.\u001b[39;49mjoin(file)\u001b[39m.\u001b[39mopen(encoding)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m stream\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/nltk/data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[0;34m(self, fileid)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(\u001b[39mself\u001b[39m, fileid):\n\u001b[1;32m    333\u001b[0m     _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, fileid)\n\u001b[0;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/nltk/compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decorator\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     40\u001b[0m     args \u001b[39m=\u001b[39m (args[\u001b[39m0\u001b[39m], add_py3_data(args[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m args[\u001b[39m2\u001b[39m:]\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m init_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/nltk/data.py:311\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \n\u001b[1;32m    307\u001b[0m \u001b[39m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(_path)\n\u001b[0;32m--> 311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mexists(_path):\n\u001b[1;32m    312\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m _path)\n\u001b[1;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m _path\n",
      "File \u001b[0;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define your parameter ranges\n",
    "remove_stopwords = True\n",
    "use_lemmatize_options = [False, True]\n",
    "tags_weighting_options = [1, 2, 5]\n",
    "ngram_range_options = [(1, 1), (1, 2), (1, 3)]\n",
    "\n",
    "# Store the results for each n_topics uniquely\n",
    "all_results = {}\n",
    "\n",
    "# Store the results for each n_topics uniquely\n",
    "all_results = {}\n",
    "\n",
    "# Iterate over the combinations of other options\n",
    "for use_lemmatize, tags_weighting, ngram_range in itertools.product(use_lemmatize_options, tags_weighting_options, ngram_range_options):\n",
    "    # Convert ngram_range tuple to string format\n",
    "    ngram_range_str = '_'.join(map(str, ngram_range))\n",
    "\n",
    "    # Construct a unique run name for this combination\n",
    "    run_name = f\"Run_remove_{remove_stopwords}_lemmatize_{use_lemmatize}_weight_{tags_weighting}_ngram_{ngram_range_str}_maxfeat_2000\"\n",
    "\n",
    "    # Run the function and get the results\n",
    "    topics_results = apply_topic_modeling_and_log(\n",
    "        questions_df, \n",
    "        remove_stopwords, \n",
    "        use_lemmatize, \n",
    "        tags_weighting, \n",
    "        run_name, \n",
    "        ngram_range, \n",
    "        max_features=2000\n",
    "    )\n",
    "\n",
    "    # Update all_results to include these structured results\n",
    "    all_results.update(topics_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a file or handle them as needed\n",
    "# For example, saving to a pickle file\n",
    "import pickle\n",
    "with open('topic_modeling_results_n_topics_SO.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for Improvements\n",
    "- Adjust StopWords?\n",
    "- **Hyperparameter Tuning**: Tune the parameters of the LDA model,\n",
    "    - learning decay\n",
    "    - batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Should only Post-Level have topic assigned to them?\n",
    "    - Then Sub-Posts are assigned the same topic as Post\n",
    "    - Comments are assigned the same topic as Post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Apply Sentiment Analysis on:\n",
    "- Post Level\n",
    "- Sub Post Level\n",
    "- Comment Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-Post-Topic Matrix**: \n",
    "- Create a matrix where rows represent users and columns represent topics. \n",
    "- Each cell contains the count of posts/comments a user has made in a particular topic.\n",
    "    - Post Level: where `PostTypeId` == 1 AND `ParentId` == -1\n",
    "    - Sub Post Level: where `PostTypeId` == 1 AND `ParentId` != -1\n",
    "    - Comment Level: where `PostTypeId` == 2\n",
    "- **Include Post Statistics**\n",
    "    - AcceptedAnswerId\n",
    "    - Score\n",
    "    - ViewCount\n",
    "    - AnswerCount\n",
    "    - CommentCount\n",
    "- **Include Comment Statistics**\n",
    "    - Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering Algorithms**\n",
    "- K-Means: Use the user-topic matrix to cluster users. Determine the optimal number of clusters (communities) using the Elbow method or Silhouette score.\n",
    "\n",
    "- Hierarchical Clustering: Useful for understanding the data structure and forming hierarchical communities. Dendrograms can visualize the community structure.\n",
    "\n",
    "- DBSCAN: Good for datasets with noise and clusters of varying densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Market Basket Analysis**\n",
    "- Association Rules and Apriori Algorithm: \n",
    "    - Treat each user's set of topics as a 'basket'. \n",
    "    - Identify strong rules where the presence of one topic implies the presence of another in a user's posts\n",
    "    - This can highlight topic-based communities.\n",
    "- Frequent Itemsets: \n",
    "    - Identify sets of topics that frequently occur together in users' posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locality Sensitive Hashing (LSH)**\n",
    "- LSH for Dimension Reduction: \n",
    "    - If the user-topic matrix is very sparse and high-dimensional, LSH can reduce dimensions while preserving the similarity structure. This can make subsequent clustering more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Techniques**\n",
    "- PCY Algorithm: If you're dealing with very large data, this algorithm efficiently finds frequent itemsets, useful in subsequent association rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Davies-Bouldin Index**: Evaluate the quality of clusters. \n",
    "- Lower Davies-Bouldin index values signify better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
