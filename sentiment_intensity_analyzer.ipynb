{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import html\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df_comments = pd.read_pickle('./pickle_dataframes/comments_typecasted.pkl')\n",
    "df_posts = pd.read_pickle('./pickle_dataframes/posts_typecasted.pkl')\n",
    "\n",
    "df_postlinks = pd.read_pickle('./pickle_dataframes/post_links_typecasted.pkl')\n",
    "df_tags = pd.read_pickle('./pickle_dataframes/tags_typecasted.pkl')\n",
    "df_users = pd.read_pickle('./pickle_dataframes/users_typecasted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter active users (>50 posts/comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering posts to include only questions\n",
    "questions_df = df_posts[df_posts['PostTypeId'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing entries with -1 in UserId and OwnerUserId columns\n",
    "df_comments = df_comments[df_comments['UserId'] != -1]\n",
    "df_posts = df_posts[df_posts['OwnerUserId'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating post and comment counts for each user\n",
    "user_posts_count = df_posts.groupby('OwnerUserId').size().rename('PostCount')\n",
    "user_comments_count = df_comments.groupby('UserId').size().rename('CommentCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging counts with user data and filling missing values\n",
    "user_data = df_users.merge(user_posts_count, left_on='Id', right_index=True, how='left') \\\n",
    "                    .merge(user_comments_count, left_on='Id', right_index=True, how='left') \\\n",
    "                    .fillna({'PostCount': 0, 'CommentCount': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column for total activity and filtering for active users\n",
    "active_users = user_data.assign(TotalActivity=lambda x: x['PostCount'] + x['CommentCount'])\n",
    "active_users = active_users[active_users['TotalActivity'] > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a set of active user IDs\n",
    "active_user_ids = set(active_users['Id'])\n",
    "\n",
    "# Filtering dataframes for active user activity\n",
    "filtered_questions_df = questions_df[questions_df['OwnerUserId'].isin(active_user_ids)]\n",
    "\n",
    "# Combining filters for comments related to active users\n",
    "active_user_post_ids = set(df_posts[df_posts['OwnerUserId'].isin(active_user_ids)]['Id'])\n",
    "\n",
    "# Filtering for answers by active users\n",
    "active_user_answers = df_posts[(df_posts['PostTypeId'] == 2) & (df_posts['OwnerUserId'].isin(active_user_ids))]\n",
    "\n",
    "# Getting ParentIds of answers by active users\n",
    "active_user_answer_parent_ids = set(active_user_answers['ParentId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the logic for filtered comments\n",
    "filtered_comments = df_comments[(df_comments['UserId'].isin(active_user_ids)) |\n",
    "                                (df_comments['PostId'].isin(active_user_post_ids)) |\n",
    "                                (df_comments['PostId'].isin(active_user_answer_parent_ids))].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing sizes for comparison\n",
    "print(\"Questions DataFrame size:\", questions_df.size)\n",
    "print(\"Filtered Questions DataFrame size:\", filtered_questions_df.size, '\\n')\n",
    "\n",
    "print(\"Comments DataFrame size:\", df_comments.size)\n",
    "print(\"Filtered Comments DataFrame size:\", filtered_comments.size, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify preprocess_text function\n",
    "def preprocess_text(text, remove_stopwords=False, use_lemmatize=True, use_stemmer=False):\n",
    "    # Handle None or non-string inputs\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower())\n",
    "\n",
    "    words = text.split()\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "    if use_lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    elif use_stemmer:  # Apply stemming only if use_stemmer is True\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Title, Body, and Tags using Dask DataFrames\n",
    "# Convert the DFs to Dask DataFrames\n",
    "ddf_comments = dd.from_pandas(filtered_questions_df, npartitions=8)\n",
    "ddf_posts = dd.from_pandas(filtered_questions_df, npartitions=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Columns\n",
    "ddf_comments['Text_Processed'] = ddf_comments['Text'].map_partitions(lambda x: x.apply(lambda y: preprocess_text(y, remove_stopwords=True, use_lemmatize=True, use_stemmer=False)))\n",
    "\n",
    "# Convert back to pandas DataFrame and save as pickle\n",
    "df_comments_processed = ddf_comments.compute()\n",
    "df_comments_processed.to_pickle('df_comments_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Posts\n",
    "ddf_posts['Title_Processed'] = ddf_posts['Title'].map_partitions(lambda x: x.apply(lambda y: preprocess_text(y, remove_stopwords=True, use_lemmatize=True, use_stemmer=False)))\n",
    "ddf_posts['Body_Processed'] = ddf_posts['Body'].map_partitions(lambda x: x.apply(lambda y: preprocess_text(y, remove_stopwords=True, use_lemmatize=True, use_stemmer=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to pandas DataFrame and save as pickle\n",
    "df_posts_processed = ddf_posts.compute()\n",
    "df_posts_processed.to_pickle('df_posts_processed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SentimentIntensityAnalyzer once\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to apply sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    # Check if the text is missing or NaN, return 0.0 in such cases\n",
    "    if pd.isna(text):\n",
    "        return 0.0\n",
    "    # Ensure the text is encoded as a string\n",
    "    text = str(text)\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_data(df, npartitions=None):\n",
    "#     start_time = time.time()\n",
    "#     \n",
    "#     # If npartitions is not specified, default to 1 (suitable for sequential processing)\n",
    "#     if npartitions is None:\n",
    "#         npartitions = 1\n",
    "# \n",
    "#     # Convert DataFrame to Dask DataFrame\n",
    "#     ddf = dd.from_pandas(df, npartitions=npartitions)\n",
    "#     # Apply sentiment analysis\n",
    "#     ddf['sentiment'] = ddf['Body'].map(analyze_sentiment, meta=('Body', 'float64'))\n",
    "# \n",
    "#     # Compute result and monitor memory usage\n",
    "#     result = ddf.compute()\n",
    "#     memory_usage = psutil.virtual_memory()\n",
    "#     \n",
    "#     end_time = time.time()\n",
    "#     return result, end_time - start_time, memory_usage.used\n",
    "\n",
    "# Sequential processing (no parallelism)\n",
    "# print(\"Running sequentially...\")\n",
    "# seq_result, seq_time, _ = process_data(df_posts)\n",
    "# print(f\"Sequential processing time: {seq_time} seconds\")\n",
    "# \n",
    "# # Parallel processing with multiple cores\n",
    "# core_counts = [2, 4, 6, 7, 8, 10]\n",
    "# for cores in core_counts:\n",
    "#     print(f\"Running with {cores} cores...\")\n",
    "#     with Client(n_workers=cores, threads_per_worker=2) as client:  # Adjust threads_per_worker as needed\n",
    "#         _, parallel_time, mem_usage = process_data(df_posts, npartitions=cores)\n",
    "#         efficiency = seq_time / (cores * parallel_time)\n",
    "#         print(f\"Time with {cores} cores: {parallel_time} seconds, Efficiency: {efficiency}, Memory used: {mem_usage} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrame to Dask DataFrame\n",
    "df_comments_dask = dd.from_pandas(df_comments_processed, npartitions=8)  # Adjust npartitions based on memory usage results\n",
    "df_posts_dask = dd.from_pandas(df_posts_processed, npartitions=8)  # Adjust npartitions based on memory usage results\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df_comments_dask['Text_sentiment'] = df_comments_dask['Text'].map(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_dask['body_sentiment'] = df_posts_dask['Body'].map(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_dask['title_sentiment'] = df_posts_dask['Title'].map(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 27m 22s\n"
     ]
    }
   ],
   "source": [
    "# Compute the results with progress bar\n",
    "with ProgressBar():\n",
    "    df_comments_result = ddf_comments.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 21m 39s\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    df_posts_result = ddf_posts.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_result.to_pickle('df_comments_result.pkl')\n",
    "df_posts_result.to_pickle('df_posts_result.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
