{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "import html\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './pickle_dataframes/comments_typecasted.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\pih\\OneDrive - Capital Four Management Fondsm√¶glerselskab A S\\notesPH\\personal\\school\\dtu\\compToolsForDS\\stackoverflow\\compToolsForDSProject\\topic_modelling_freq_itemsets.ipynb Cell 2\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pih/OneDrive%20-%20Capital%20Four%20Management%20Fondsm%C3%A6glerselskab%20A%20S/notesPH/personal/school/dtu/compToolsForDS/stackoverflow/compToolsForDSProject/topic_modelling_freq_itemsets.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Read the data\u001b[39;00m\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pih/OneDrive%20-%20Capital%20Four%20Management%20Fondsm%C3%A6glerselskab%20A%20S/notesPH/personal/school/dtu/compToolsForDS/stackoverflow/compToolsForDSProject/topic_modelling_freq_itemsets.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_c \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_pickle(\u001b[39m'\u001b[39;49m\u001b[39m./pickle_dataframes/comments_typecasted.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pih/OneDrive%20-%20Capital%20Four%20Management%20Fondsm%C3%A6glerselskab%20A%20S/notesPH/personal/school/dtu/compToolsForDS/stackoverflow/compToolsForDSProject/topic_modelling_freq_itemsets.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_p \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m'\u001b[39m\u001b[39m./pickle_dataframes/posts_typecasted.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pih/OneDrive%20-%20Capital%20Four%20Management%20Fondsm%C3%A6glerselskab%20A%20S/notesPH/personal/school/dtu/compToolsForDS/stackoverflow/compToolsForDSProject/topic_modelling_freq_itemsets.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df_pl \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m'\u001b[39m\u001b[39m./pickle_dataframes/post_links_typecasted.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\pickle.py:179\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n",
      "\u001b[0;32m    115\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    116\u001b[0m \u001b[39mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n",
      "\u001b[0;32m    117\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    176\u001b[0m \u001b[39m4    4    9\u001b[39;00m\n",
      "\u001b[0;32m    177\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    178\u001b[0m excs_to_catch \u001b[39m=\u001b[39m (\u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mModuleNotFoundError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m)\n",
      "\u001b[1;32m--> 179\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n",
      "\u001b[0;32m    180\u001b[0m     filepath_or_buffer,\n",
      "\u001b[0;32m    181\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m    182\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n",
      "\u001b[0;32m    183\u001b[0m     is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m    184\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n",
      "\u001b[0;32m    185\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n",
      "\u001b[0;32m    186\u001b[0m     \u001b[39m# 1) try standard library Pickle\u001b[39;00m\n",
      "\u001b[0;32m    187\u001b[0m     \u001b[39m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n",
      "\u001b[0;32m    188\u001b[0m     \u001b[39m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n",
      "\u001b[0;32m    190\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m    191\u001b[0m         \u001b[39m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n",
      "\u001b[0;32m    192\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n",
      "\u001b[0;32m    859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n",
      "\u001b[0;32m    860\u001b[0m             handle,\n",
      "\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[0;32m    865\u001b[0m         )\n",
      "\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n",
      "\u001b[1;32m--> 868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n",
      "\u001b[0;32m    869\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n",
      "\u001b[0;32m    871\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pickle_dataframes/comments_typecasted.pkl'"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "df_c = pd.read_pickle('./pickle_dataframes/comments_typecasted.pkl')\n",
    "df_p = pd.read_pickle('./pickle_dataframes/posts_typecasted.pkl')\n",
    "\n",
    "df_pl = pd.read_pickle('./pickle_dataframes/post_links_typecasted.pkl')\n",
    "df_t = pd.read_pickle('./pickle_dataframes/tags_typecasted.pkl')\n",
    "df_u = pd.read_pickle('./pickle_dataframes/users_typecasted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = df_c.copy()\n",
    "df_posts = df_p.copy()\n",
    "#df_post_links = df_p.copy()\n",
    "#df_tags = df_t.copy()\n",
    "df_users = df_u.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-03 09:33:40.880000\n"
     ]
    }
   ],
   "source": [
    "max_date_comments = df_comments[\"CreationDate\"].max()\n",
    "max_date_posts = df_posts[\"CreationDate\"].max()\n",
    "\n",
    "# Use the latest date as the reference for the n-year filter\n",
    "max_date = max(max_date_comments, max_date_posts)\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic model: only posts from active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Users:  1116787\n",
      "Users before Activity Threshold:  1116805\n",
      "Users after Activity Threshold:  34943\n"
     ]
    }
   ],
   "source": [
    "# Remove entries with -1 in UserId and OwnerUserId columns\n",
    "df_comments = df_comments[df_comments['UserId'] != -1]\n",
    "df_posts = df_posts[df_posts['OwnerUserId'] != -1]\n",
    "\n",
    "# Selecting only questions from posts\n",
    "questions_df = df_posts[df_posts['PostTypeId'] == 1]\n",
    "answers_df = df_posts[df_posts['PostTypeId'] == 2]\n",
    "\n",
    "#print(\"Number of comments \", df_comments.shape[0])\n",
    "#print(\"Number of posts \", df_posts.shape[0])\n",
    "#print(\"Number of questions \", questions_df.shape[0])\n",
    "#print(\"Number of answers \", answers_df.shape[0])\n",
    "\n",
    "# Count unique users involved in questions and comments\n",
    "unique_user_count = len(pd.concat([questions_df[\"OwnerUserId\"], answers_df[\"OwnerUserId\"], df_comments[\"UserId\"]]).unique())\n",
    "# print(\"Number of Unique Users: \", unique_user_count)\n",
    "\n",
    "# Calculate post and comment counts for each user\n",
    "user_posts_count = df_posts.groupby('OwnerUserId').size().rename('PostCount')\n",
    "user_comments_count = df_comments.groupby('UserId').size().rename('CommentCount')\n",
    "\n",
    "# Merge counts with user data and fill missing values with 0\n",
    "user_data = df_users.merge(user_posts_count, left_on='Id', right_index=True, how='left')\n",
    "user_data = user_data.merge(user_comments_count, left_on='Id', right_index=True, how='left')\n",
    "user_data.fillna({'PostCount': 0, 'CommentCount': 0}, inplace=True)\n",
    "\n",
    "# Add a column for total activity and filter for active users\n",
    "user_data['TotalActivity'] = user_data['PostCount'] + user_data['CommentCount']\n",
    "\n",
    "# Get sets of active user IDs before Activity Threshold\n",
    "print(\"Users before Activity Threshold: \" , len(set(user_data['Id'])))\n",
    "\n",
    "# Get sets of active user IDs after Activity Threshold\n",
    "active_users = user_data[user_data['TotalActivity'] > 200]\n",
    "active_user_ids = set(active_users['Id'])\n",
    "print(\"Users after Activity Threshold: \" ,len(active_user_ids))\n",
    "\n",
    "# Filter questions and comments for active user activity\n",
    "filtered_questions_df = questions_df[questions_df['OwnerUserId'].isin(active_user_ids)]\n",
    "active_user_post_ids = set(df_posts[df_posts['OwnerUserId'].isin(active_user_ids)]['Id'])\n",
    "filtered_comments = df_comments[(df_comments['UserId'].isin(active_user_ids)) | \n",
    "                                (df_comments['PostId'].isin(active_user_post_ids))].drop_duplicates()\n",
    "\n",
    "# Count unique users in filtered questions and comments\n",
    "unique_active_user_count = len(pd.concat([filtered_questions_df[\"OwnerUserId\"], filtered_comments[\"UserId\"]]).unique())\n",
    "\n",
    "#print(\"Number of comments \", filtered_comments.shape[0])\n",
    "#print(\"Number of posts \", active_user_post_ids.shape[0])\n",
    "#print(\"Number of Unique Users: \", unique_active_user_count)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
