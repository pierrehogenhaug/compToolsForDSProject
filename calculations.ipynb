{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts loaded\n",
      "Answers loaded\n",
      "Comments loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "directory = './picklefiles'\n",
    "# List all files that start with the specified pattern\n",
    "\n",
    "def load_pickle_files(directory, file_pattern):\n",
    "    matching_files = [file for file in os.listdir(directory) if file.startswith(file_pattern)]\n",
    "    matching_files\n",
    "    l = []\n",
    "    for file in matching_files:\n",
    "        l.append(pd.read_pickle(os.path.join(directory,file)))\n",
    "    return pd.concat(l)\n",
    "\n",
    "import pickle\n",
    "posts = pickle.load(open(f'{directory}/posts_with_topic.pkl', 'rb'))\n",
    "print(\"Posts loaded\")\n",
    "\n",
    "answers = load_pickle_files(directory, 'posts_typecasted_')\n",
    "answers = answers[answers[\"PostTypeId\"] == 2]\n",
    "print(\"Answers loaded\")\n",
    "\n",
    "comments = pickle.load(open(f'{directory}/comments_typecasted.pkl', 'rb'))\n",
    "print(\"Comments loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag counts loaded\n",
      "Topic tags loaded\n"
     ]
    }
   ],
   "source": [
    "TAG_COUNTS = pickle.load(open(f'{directory}/tag_counts.pkl', 'rb'))\n",
    "print(\"Tag counts loaded\")\n",
    "# Dictionary TOPIC -> {TAGS}\n",
    "TOPIC_TAGS = pickle.load(open(f'{directory}/communities_louvain.pkl', 'rb'))\n",
    "TOPIC_TAGS = [{key: value for (key,value) in topic} for topic in TOPIC_TAGS] # Convert to list of dictionaries\n",
    "print(\"Topic tags loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS = pickle.load(open(f'{directory}/active_users_with_sentiment.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_of_nan(column):\n",
    "    return round(100*sum(column.isna())/len(column),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_mapping = dict(zip(posts['Id'], posts['ViewCount']))\n",
    "answers_mapping = dict(zip(answers['Id'], answers['ViewCount']))\n",
    "\n",
    "all_mapping = {**posts_mapping, **answers_mapping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = answers[answers['ParentId'].map(posts_mapping).notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments[comments['PostId'].map(all_mapping).notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_of_nan(comments['PostId'].map(all_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6435484"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_topic_mapping = dict(zip(posts['Id'], posts['Topic']))\n",
    "answers['Topic'] = answers['ParentId'].map(question_topic_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_topic_mapping = dict(zip(answers['Id'], answers['Topic']))\n",
    "\n",
    "posts_topic_mapping = {**question_topic_mapping, **answer_topic_mapping} \n",
    "\n",
    "comments['Topic'] = comments['PostId'].map(posts_topic_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(answers, open(f'{directory}/answers_clean.pkl', 'wb'))\n",
    "pickle.dump(comments, open(f'{directory}/comments_clean.pkl', 'wb'))\n",
    "pickle.dump(TOPIC_TAGS, open(f'{directory}/topic_tags_clean.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views and ViewCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_probability = 0.5\n",
    "\n",
    "posts_views_mapping = dict(zip(posts['Id'], posts['ViewCount']*post_probability))\n",
    "answers['ViewCount'] = answers['ParentId'].map(posts_views_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_probability = 0.7\n",
    "posts_views_mapping = dict(zip(posts['Id'], posts['ViewCount']*comment_probability))\n",
    "answers_views_mapping = dict(zip(answers['Id'], answers['ViewCount']*comment_probability))\n",
    "comments['Views'] = comments['PostId'].map({**posts_views_mapping, **answers_views_mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a dictionary that links each tag to their corresponding topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag topics built\n"
     ]
    }
   ],
   "source": [
    "def find_topic(tag):\n",
    "    for i, topic in enumerate(TOPIC_TAGS):\n",
    "        if tag in topic:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "# Dictionary TAG -> TOPIC\n",
    "TAG_TOPICS = {tag: find_topic(tag) for tag in TAG_COUNTS}\n",
    "print(\"Tag topics built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get main topic for a post (the topic that we will consider to analyze the post). We follow this criteria:\n",
    "1. The main topic is the one with the most amount of tags.\n",
    "2. If there are more than one, the tags with less instances gets picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_topic(row, verbosity = 0):\n",
    "    topic_tag_counts = row[\"Topic\"]    \n",
    "    max_tag = max(topic_tag_counts)\n",
    "    count_max = sum(1 for el2 in [el == max_tag for el in topic_tag_counts] if el2)\n",
    "    if verbosity > 0:\n",
    "        print(f\"Max: {max_tag} - Count: {count_max}\")\n",
    "    if count_max == 0:\n",
    "        return None\n",
    "    elif count_max == 1:\n",
    "        return np.argmax(topic_tag_counts)\n",
    "    else:\n",
    "        if verbosity > 0:\n",
    "            print(\"else\")\n",
    "        # WHICH TAGS ARE IN THE TOPICS THAT ARE DRAWN\n",
    "        # WE SUM THE NUMBER OF TAG INSTANCES IN EACH TOPIC -> ITERATE OVER TOPIC -> ITERATE OVER TAG -> SUM -> RETURN THE TOPIC WITH THE MINIMUM VALUE\n",
    "        drawn_topics = [i for (i, tag_count) in enumerate(topic_tag_counts) if tag_count == max_tag]\n",
    "        topic_tag = {i: [tag for tag in row[\"Tags\"] if tag in TOPIC_TAGS[i]] for i in drawn_topics}\n",
    "        instance_count = {i: sum([TAG_COUNTS[tag] for tag in topic_tag[i]]) for i in drawn_topics}\n",
    "        topic = min(instance_count, key=instance_count.get)\n",
    "        return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {\"Topic\": [1,0,1,0,0,0,0,0,0,0], \"Tags\": [\"do-while\", \"android\"]}\n",
    "get_main_topic(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'posts' is your DataFrame and 'get_main_topic' is your function\n",
    "\n",
    "# Function to apply in parallel\n",
    "def apply_parallel(df_chunk):\n",
    "    return df_chunk.apply(lambda row: get_main_topic(row), axis=1)\n",
    "\n",
    "# Split DataFrame into chunks\n",
    "def split_dataframe(df, num_partitions):\n",
    "    return np.array_split(df, num_partitions)\n",
    "\n",
    "# Parallelize the apply function\n",
    "def parallelize_dataframe(df, func, num_partitions):\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        df_split = split_dataframe(df, num_partitions)\n",
    "        results = list(tqdm(pool.imap(func, df_split), total=len(df_split), desc=\"Processing\"))\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Apply the function in parallel\n",
    "num_partitions = mp.cpu_count()  # Number of partitions to split dataframe\n",
    "posts['MainTopic'] = parallelize_dataframe(posts, apply_parallel, num_partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts['MainTopic'] = posts.apply(lambda row : get_main_topic(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the topic of parent post to the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_maintopic_mapping = dict(zip(posts['Id'], posts['MainTopic']))\n",
    "answers['MainTopic'] = answers['ParentId'].map(question_maintopic_mapping)\n",
    "print(\"Answers topic assigned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the topic of parent post or answer to the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_maintopic_mapping = dict(zip(answers['Id'], answers['MainTopic']))\n",
    "\n",
    "posts_topic_mapping = {**question_maintopic_mapping, **answer_maintopic_mapping} \n",
    "\n",
    "comments['MainTopic'] = comments['PostId'].map(posts_topic_mapping)\n",
    "print(\"Comments topic assigned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(sentiment, i):\n",
    "    if type(sentiment) == list:\n",
    "        return sentiment[i]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original method\n",
    "def scoring(topic, weights, row):\n",
    "    userAnswers = answers[(answers[\"OwnerUserId\"] == row['Id']) and (answers[\"Topic\"] == topic)]\n",
    "    userComments = comments[(comments[\"UserId\"] == row['Id']) and (comments[\"Topic\"] == topic)]\n",
    "    userPosts = posts[(posts[\"OwnerUserId\"] == row['Id']) and (posts[\"Topic\"] == topic)]\n",
    "    return len(userAnswers) * (get_sentiment(row[\"AnswerSentiment\"], topic) * 20*sum(userAnswers[\"Score\"])/sum(userAnswers[\"ViewCount\"]) ) * weights[0] +\\\n",
    "        len(userComments) * min(sum(userComments[\"Score\"])/sum(userComments[\"Views\"]),1) * weights[1] +\\\n",
    "        min(sum(userPosts[\"Score\"])/sum(userPosts[\"Views\"]),1) * weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Topic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\CTDS\\compToolsForDSProject\\calculations.ipynb Cell 32\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CTDS/compToolsForDSProject/calculations.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m WEIGHTS \u001b[39m=\u001b[39m [\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CTDS/compToolsForDSProject/calculations.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m answers_grouped \u001b[39m=\u001b[39m answers\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mOwnerUserId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mViewCount\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mId\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msize\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CTDS/compToolsForDSProject/calculations.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m comments_grouped \u001b[39m=\u001b[39m comments\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mUserId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mViews\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mId\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msize\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CTDS/compToolsForDSProject/calculations.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m posts_grouped \u001b[39m=\u001b[39m posts\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mOwnerUserId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mViews\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mId\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msize\u001b[39m\u001b[39m'\u001b[39m})\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8399\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   8400\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8402\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8403\u001b[0m     obj\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m   8404\u001b[0m     keys\u001b[39m=\u001b[39mby,\n\u001b[0;32m   8405\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m   8406\u001b[0m     level\u001b[39m=\u001b[39mlevel,\n\u001b[0;32m   8407\u001b[0m     as_index\u001b[39m=\u001b[39mas_index,\n\u001b[0;32m   8408\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m   8409\u001b[0m     group_keys\u001b[39m=\u001b[39mgroup_keys,\n\u001b[0;32m   8410\u001b[0m     squeeze\u001b[39m=\u001b[39msqueeze,\n\u001b[0;32m   8411\u001b[0m     observed\u001b[39m=\u001b[39mobserved,\n\u001b[0;32m   8412\u001b[0m     dropna\u001b[39m=\u001b[39mdropna,\n\u001b[0;32m   8413\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrouper\u001b[39;00m \u001b[39mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[0;32m    966\u001b[0m         obj,\n\u001b[0;32m    967\u001b[0m         keys,\n\u001b[0;32m    968\u001b[0m         axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m    969\u001b[0m         level\u001b[39m=\u001b[39mlevel,\n\u001b[0;32m    970\u001b[0m         sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m    971\u001b[0m         observed\u001b[39m=\u001b[39mobserved,\n\u001b[0;32m    972\u001b[0m         mutated\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutated,\n\u001b[0;32m    973\u001b[0m         dropna\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropna,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    976\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[0;32m    977\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Topic'"
     ]
    }
   ],
   "source": [
    "WEIGHTS = [0.5, 0.5, 0]\n",
    "\n",
    "\n",
    "answers_grouped = answers.groupby(['OwnerUserId', 'Topic']).agg({'Score': 'sum', 'ViewCount': 'sum', 'Id': 'size'})\n",
    "comments_grouped = comments.groupby(['UserId', 'Topic']).agg({'Score': 'sum', 'Views': 'sum', 'Id': 'size'})\n",
    "posts_grouped = posts.groupby(['OwnerUserId', 'Topic']).agg({'Score': 'sum', 'Views': 'sum', 'Id': 'size'})\n",
    "\n",
    "def scoring(topic, weights, row):\n",
    "    # Retrieve data for the specific user and topic, if available\n",
    "    userAnswers = answers_grouped.loc[(row['Id'], topic)] if (row['Id'], topic) in answers_grouped.index else pd.Series({'Score': 0, 'ViewCount': 1, 'Id': 0})\n",
    "    userComments = comments_grouped.loc[(row['UserId'], topic)] if (row['UserId'], topic) in comments_grouped.index else pd.Series({'Score': 0, 'Views': 1, 'Id': 0})\n",
    "    userPosts = posts_grouped.loc[(row['Id'], topic)] if (row['Id'], topic) in posts_grouped.index else pd.Series({'Score': 0, 'Views': 1, 'Id': 0})\n",
    "\n",
    "    # Calculate the score\n",
    "    score = userAnswers['Id'] * (get_sentiment(row[\"AnswerSentiment\"], topic) * 20 * userAnswers['Score'] / userAnswers['ViewCount']) * weights[0] +\\\n",
    "            userComments['Id'] * max(userComments['Score'] / userComments['Views'], 1) * weights[1] +\\\n",
    "            max(userPosts['Score'] / userPosts['Views'], 1) * weights[2]\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USERS[\"Score\"] = USERS.apply(lambda row: [scoring(i, WEIGHTS, row) for i,_ in enumerate(TOPIC_TAGS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Convert the pandas DataFrame to a Dask DataFrame\n",
    "dask_users = dd.from_pandas(USERS, npartitions=8) # Adjust the number of partitions based on your dataset\n",
    "\n",
    "# Define a lambda function to be applied to each row\n",
    "lambda_function = lambda row: [scoring(i, WEIGHTS, row) for i, _ in enumerate(TOPIC_TAGS)]\n",
    "\n",
    "# Apply the function\n",
    "# Specify meta as a list, since the function returns a list\n",
    "dask_users['Score'] = dask_users.apply(lambda_function, axis=1, meta=('Score', 'object'))\n",
    "\n",
    "# Compute the results to get back a pandas DataFrame with a progress bar\n",
    "with ProgressBar():\n",
    "    USERS = dask_users.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sg_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
