{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts loaded\n",
      "Answers loaded\n",
      "Comments loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "directory = './picklefiles'\n",
    "# List all files that start with the specified pattern\n",
    "\n",
    "def load_pickle_files(directory, file_pattern):\n",
    "    matching_files = [file for file in os.listdir(directory) if file.startswith(file_pattern)]\n",
    "    matching_files\n",
    "    l = []\n",
    "    for file in matching_files:\n",
    "        l.append(pd.read_pickle(os.path.join(directory,file)))\n",
    "    return pd.concat(l)\n",
    "\n",
    "import pickle\n",
    "posts = pickle.load(open(f'{directory}/posts_with_topic.pkl', 'rb'))\n",
    "print(\"Posts loaded\")\n",
    "\n",
    "answers = load_pickle_files(directory, 'posts_typecasted_')\n",
    "answers = answers[answers[\"PostTypeId\"] == 2]\n",
    "print(\"Answers loaded\")\n",
    "\n",
    "comments = pickle.load(open(f'{directory}/comments_typecasted.pkl', 'rb'))\n",
    "print(\"Comments loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag counts loaded\n",
      "Topic tags loaded\n"
     ]
    }
   ],
   "source": [
    "TAG_COUNTS = pickle.load(open(f'{directory}/tag_counts.pkl', 'rb'))\n",
    "print(\"Tag counts loaded\")\n",
    "# Dictionary TOPIC -> {TAGS}\n",
    "TOPIC_TAGS = pickle.load(open(f'{directory}/communities_louvain.pkl', 'rb'))\n",
    "TOPIC_TAGS = [{key: value for (key,value) in topic} for topic in TOPIC_TAGS] # Convert to list of dictionaries\n",
    "print(\"Topic tags loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS = pickle.load(open(f'{directory}/active_users_with_sentiment.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_of_nan(column):\n",
    "    return round(100*sum(column.isna())/len(column),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_mapping = dict(zip(posts['Id'], posts['ViewCount']))\n",
    "answers_mapping = dict(zip(answers['Id'], answers['ViewCount']))\n",
    "\n",
    "all_mapping = {**posts_mapping, **answers_mapping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = answers[answers['ParentId'].map(posts_mapping).notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments[comments['PostId'].map(all_mapping).notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_of_nan(comments['PostId'].map(all_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6435484"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_topic_mapping = dict(zip(posts['Id'], posts['Topic']))\n",
    "answers['Topic'] = answers['ParentId'].map(question_topic_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_topic_mapping = dict(zip(answers['Id'], answers['Topic']))\n",
    "\n",
    "posts_topic_mapping = {**question_topic_mapping, **answer_topic_mapping} \n",
    "\n",
    "comments['Topic'] = comments['PostId'].map(posts_topic_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(answers, open(f'{directory}/answers_clean.pkl', 'wb'))\n",
    "pickle.dump(comments, open(f'{directory}/comments_clean.pkl', 'wb'))\n",
    "pickle.dump(TOPIC_TAGS, open(f'{directory}/topic_tags_clean.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views and ViewCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_probability = 0.5\n",
    "\n",
    "posts_views_mapping = dict(zip(posts['Id'], posts['ViewCount']*post_probability))\n",
    "answers['ViewCount'] = answers['ParentId'].map(posts_views_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_probability = 0.7\n",
    "posts_views_mapping = dict(zip(posts['Id'], posts['ViewCount']*comment_probability))\n",
    "answers_views_mapping = dict(zip(answers['Id'], answers['ViewCount']*comment_probability))\n",
    "comments['Views'] = comments['PostId'].map({**posts_views_mapping, **answers_views_mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pickle.load(open(f'{directory}/answers_clean.pkl', 'rb'))\n",
    "print(\"Posts loaded\")\n",
    "comments = pickle.load(open(f'{directory}/comments_clean.pkl', 'rb'))\n",
    "print(\"Answers loaded\")\n",
    "TOPIC_TAGS = pickle.load(open(f'{directory}/topic_tags_clean.pkl', 'rb'))\n",
    "print(\"Topic tags loaded\")\n",
    "TAG_COUNTS = pickle.load(open(f'{directory}/tag_counts.pkl', 'rb'))\n",
    "print(\"Tag counts loaded\")\n",
    "USERS = pickle.load(open(f'{directory}/active_users_with_sentiment.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a dictionary that links each tag to their corresponding topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag topics built\n"
     ]
    }
   ],
   "source": [
    "def find_topic(tag):\n",
    "    for i, topic in enumerate(TOPIC_TAGS):\n",
    "        if tag in topic:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "# Dictionary TAG -> TOPIC\n",
    "TAG_TOPICS = {tag: find_topic(tag) for tag in TAG_COUNTS}\n",
    "print(\"Tag topics built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get main topic for a post (the topic that we will consider to analyze the post). We follow this criteria:\n",
    "1. The main topic is the one with the most amount of tags.\n",
    "2. If there are more than one, the tags with less instances gets picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_topic(row, verbosity = 0):\n",
    "    topic_tag_counts = row[\"Topic\"]    \n",
    "    max_tag = max(topic_tag_counts)\n",
    "    count_max = sum(1 for el2 in [el == max_tag for el in topic_tag_counts] if el2)\n",
    "    if verbosity > 0:\n",
    "        print(f\"Max: {max_tag} - Count: {count_max}\")\n",
    "    if count_max == 0:\n",
    "        return None\n",
    "    elif count_max == 1:\n",
    "        return np.argmax(topic_tag_counts)\n",
    "    else:\n",
    "        if verbosity > 0:\n",
    "            print(\"else\")\n",
    "        # WHICH TAGS ARE IN THE TOPICS THAT ARE DRAWN\n",
    "        # WE SUM THE NUMBER OF TAG INSTANCES IN EACH TOPIC -> ITERATE OVER TOPIC -> ITERATE OVER TAG -> SUM -> RETURN THE TOPIC WITH THE MINIMUM VALUE\n",
    "        drawn_topics = [i for (i, tag_count) in enumerate(topic_tag_counts) if tag_count == max_tag]\n",
    "        topic_tag = {i: [tag for tag in row[\"Tags\"] if tag in TOPIC_TAGS[i]] for i in drawn_topics}\n",
    "        instance_count = {i: sum([TAG_COUNTS[tag] for tag in topic_tag[i]]) for i in drawn_topics}\n",
    "        topic = min(instance_count, key=instance_count.get)\n",
    "        return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {\"Topic\": [1,0,1,0,0,0,0,0,0,0], \"Tags\": [\"do-while\", \"android\"]}\n",
    "get_main_topic(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'posts' is your DataFrame and 'get_main_topic' is your function\n",
    "\n",
    "# Function to apply in parallel\n",
    "def apply_parallel(df_chunk):\n",
    "    return df_chunk.apply(lambda row: get_main_topic(row), axis=1)\n",
    "\n",
    "# Split DataFrame into chunks\n",
    "def split_dataframe(df, num_partitions):\n",
    "    return np.array_split(df, num_partitions)\n",
    "\n",
    "# Parallelize the apply function\n",
    "def parallelize_dataframe(df, func, num_partitions):\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        df_split = split_dataframe(df, num_partitions)\n",
    "        results = list(tqdm(pool.imap(func, df_split), total=len(df_split), desc=\"Processing\"))\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Apply the function in parallel\n",
    "num_partitions = mp.cpu_count()  # Number of partitions to split dataframe\n",
    "posts['MainTopic'] = parallelize_dataframe(posts, apply_parallel, num_partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts['MainTopic'] = posts.apply(lambda row : get_main_topic(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the topic of parent post to the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_maintopic_mapping = dict(zip(posts['Id'], posts['MainTopic']))\n",
    "answers['MainTopic'] = answers['ParentId'].map(question_maintopic_mapping)\n",
    "print(\"Answers topic assigned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the topic of parent post or answer to the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_maintopic_mapping = dict(zip(answers['Id'], answers['MainTopic']))\n",
    "\n",
    "posts_topic_mapping = {**question_maintopic_mapping, **answer_maintopic_mapping} \n",
    "\n",
    "comments['MainTopic'] = comments['PostId'].map(posts_topic_mapping)\n",
    "print(\"Comments topic assigned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(sentiment, i):\n",
    "    if type(sentiment) == list:\n",
    "        return sentiment[i]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original method\n",
    "\n",
    "def scoring(topic, weights, row):\n",
    "    userAnswers = answers[(answers[\"OwnerUserId\"] == row['Id']) and (answers[\"MainTopic\"] == topic)]\n",
    "    userComments = comments[(comments[\"UserId\"] == row['Id']) and (comments[\"MainTopic\"] == topic)]\n",
    "    userPosts = posts[(posts[\"OwnerUserId\"] == row['Id']) and (posts[\"MainTopic\"] == topic)]\n",
    "    return len(userAnswers) * (get_sentiment(row[\"AnswerSentiment\"], topic) * 20*sum(userAnswers[\"Score\"])/sum(userAnswers[\"ViewCount\"]) ) * weights[0] +\\\n",
    "        len(userComments) * min(sum(userComments[\"Score\"])/sum(userComments[\"Views\"]),1) * weights[1] +\\\n",
    "        min(sum(userPosts[\"Score\"])/sum(userPosts[\"Views\"]),1) * weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\CTDS\\compToolsForDSProject\\calculations.ipynb Cell 37\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CTDS/compToolsForDSProject/calculations.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m WEIGHTS \u001b[39m=\u001b[39m [\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CTDS/compToolsForDSProject/calculations.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m answers_grouped \u001b[39m=\u001b[39m answers\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mOwnerUserId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mViewCount\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mId\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msize\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CTDS/compToolsForDSProject/calculations.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m comments_grouped \u001b[39m=\u001b[39m comments\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mUserId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mViews\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mId\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msize\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CTDS/compToolsForDSProject/calculations.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m posts_grouped \u001b[39m=\u001b[39m posts\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mOwnerUserId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mViews\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mId\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msize\u001b[39m\u001b[39m'\u001b[39m})\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:895\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    892\u001b[0m func \u001b[39m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[0;32m    894\u001b[0m op \u001b[39m=\u001b[39m GroupByApply(\u001b[39mself\u001b[39m, func, args, kwargs)\n\u001b[1;32m--> 895\u001b[0m result \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39magg()\n\u001b[0;32m    896\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dict_like(func) \u001b[39mand\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    897\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\apply.py:172\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m is_dict_like(arg):\n\u001b[1;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magg_dict_like()\n\u001b[0;32m    173\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(arg):\n\u001b[0;32m    174\u001b[0m     \u001b[39m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magg_list_like()\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\apply.py:504\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    501\u001b[0m     results \u001b[39m=\u001b[39m {key: colg\u001b[39m.\u001b[39magg(how) \u001b[39mfor\u001b[39;00m key, how \u001b[39min\u001b[39;00m arg\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m    502\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m     \u001b[39m# key used for column selection and output\u001b[39;00m\n\u001b[1;32m--> 504\u001b[0m     results \u001b[39m=\u001b[39m {\n\u001b[0;32m    505\u001b[0m         key: obj\u001b[39m.\u001b[39m_gotitem(key, ndim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39magg(how) \u001b[39mfor\u001b[39;00m key, how \u001b[39min\u001b[39;00m arg\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    506\u001b[0m     }\n\u001b[0;32m    508\u001b[0m \u001b[39m# set the final keys\u001b[39;00m\n\u001b[0;32m    509\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(arg\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\apply.py:505\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    501\u001b[0m     results \u001b[39m=\u001b[39m {key: colg\u001b[39m.\u001b[39magg(how) \u001b[39mfor\u001b[39;00m key, how \u001b[39min\u001b[39;00m arg\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m    502\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m     \u001b[39m# key used for column selection and output\u001b[39;00m\n\u001b[0;32m    504\u001b[0m     results \u001b[39m=\u001b[39m {\n\u001b[1;32m--> 505\u001b[0m         key: obj\u001b[39m.\u001b[39m_gotitem(key, ndim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39magg(how) \u001b[39mfor\u001b[39;00m key, how \u001b[39min\u001b[39;00m arg\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    506\u001b[0m     }\n\u001b[0;32m    508\u001b[0m \u001b[39m# set the final keys\u001b[39;00m\n\u001b[0;32m    509\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(arg\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:275\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, func)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    277\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, abc\u001b[39m.\u001b[39mIterable):\n\u001b[0;32m    278\u001b[0m     \u001b[39m# Catch instances of lists / tuples\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[39m# but not the class list / tuple itself.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     func \u001b[39m=\u001b[39m maybe_mangle_lambdas(func)\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:2456\u001b[0m, in \u001b[0;36mGroupBy.sum\u001b[1;34m(self, numeric_only, min_count, engine, engine_kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2452\u001b[0m     \u001b[39m# If we are grouping on categoricals we want unobserved categories to\u001b[39;00m\n\u001b[0;32m   2453\u001b[0m     \u001b[39m# return zero, rather than the default of NaN which the reindexing in\u001b[39;00m\n\u001b[0;32m   2454\u001b[0m     \u001b[39m# _agg_general() returns. GH #31422\u001b[39;00m\n\u001b[0;32m   2455\u001b[0m     \u001b[39mwith\u001b[39;00m com\u001b[39m.\u001b[39mtemp_setattr(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mobserved\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m-> 2456\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agg_general(\n\u001b[0;32m   2457\u001b[0m             numeric_only\u001b[39m=\u001b[39mnumeric_only,\n\u001b[0;32m   2458\u001b[0m             min_count\u001b[39m=\u001b[39mmin_count,\n\u001b[0;32m   2459\u001b[0m             alias\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2460\u001b[0m             npfunc\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msum,\n\u001b[0;32m   2461\u001b[0m         )\n\u001b[0;32m   2463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_output(result, fill_value\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1711\u001b[0m, in \u001b[0;36mGroupBy._agg_general\u001b[1;34m(self, numeric_only, min_count, alias, npfunc)\u001b[0m\n\u001b[0;32m   1699\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   1700\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_agg_general\u001b[39m(\n\u001b[0;32m   1701\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1706\u001b[0m     npfunc: Callable,\n\u001b[0;32m   1707\u001b[0m ):\n\u001b[0;32m   1709\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group_selection_context():\n\u001b[0;32m   1710\u001b[0m         \u001b[39m# try a cython aggregation if we can\u001b[39;00m\n\u001b[1;32m-> 1711\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cython_agg_general(\n\u001b[0;32m   1712\u001b[0m             how\u001b[39m=\u001b[39malias,\n\u001b[0;32m   1713\u001b[0m             alt\u001b[39m=\u001b[39mnpfunc,\n\u001b[0;32m   1714\u001b[0m             numeric_only\u001b[39m=\u001b[39mnumeric_only,\n\u001b[0;32m   1715\u001b[0m             min_count\u001b[39m=\u001b[39mmin_count,\n\u001b[0;32m   1716\u001b[0m         )\n\u001b[0;32m   1717\u001b[0m         \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1810\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m   1806\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m   1808\u001b[0m \u001b[39m# TypeError -> we may have an exception in trying to aggregate\u001b[39;00m\n\u001b[0;32m   1809\u001b[0m \u001b[39m#  continue and exclude the block\u001b[39;00m\n\u001b[1;32m-> 1810\u001b[0m new_mgr \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mgrouped_reduce(array_func, ignore_failures\u001b[39m=\u001b[39mignore_failures)\n\u001b[0;32m   1812\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_ser \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(new_mgr) \u001b[39m<\u001b[39m orig_len:\n\u001b[0;32m   1813\u001b[0m     warn_dropping_nuisance_columns_deprecated(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m), how, numeric_only)\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\internals\\base.py:199\u001b[0m, in \u001b[0;36mSingleDataManager.grouped_reduce\u001b[1;34m(self, func, ignore_failures)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39mignore_failures : bool, default False\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m    Not used; for compatibility with ArrayManager/BlockManager.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray\n\u001b[1;32m--> 199\u001b[0m res \u001b[39m=\u001b[39m func(arr)\n\u001b[0;32m    200\u001b[0m index \u001b[39m=\u001b[39m default_index(\u001b[39mlen\u001b[39m(res))\n\u001b[0;32m    202\u001b[0m mgr \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfrom_array(res, index)\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1791\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39marray_func\u001b[39m(values: ArrayLike) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[0;32m   1790\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1791\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouper\u001b[39m.\u001b[39m_cython_operation(\n\u001b[0;32m   1792\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1793\u001b[0m             values,\n\u001b[0;32m   1794\u001b[0m             how,\n\u001b[0;32m   1795\u001b[0m             axis\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mndim \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m   1796\u001b[0m             min_count\u001b[39m=\u001b[39mmin_count,\n\u001b[0;32m   1797\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1798\u001b[0m         )\n\u001b[0;32m   1799\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m   1800\u001b[0m         \u001b[39m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[0;32m   1801\u001b[0m         \u001b[39m# and non-applicable functions\u001b[39;00m\n\u001b[0;32m   1802\u001b[0m         \u001b[39m# try to python agg\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m         \u001b[39m# TODO: shouldn't min_count matter?\u001b[39;00m\n\u001b[0;32m   1804\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agg_py_fallback(values, ndim\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mndim, alt\u001b[39m=\u001b[39malt)\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:1035\u001b[0m, in \u001b[0;36mBaseGrouper._cython_operation\u001b[1;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[39mReturns the values of a cython operation.\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[39massert\u001b[39;00m kind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mtransform\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m-> 1035\u001b[0m cy_op \u001b[39m=\u001b[39m WrappedCythonOp(kind\u001b[39m=\u001b[39mkind, how\u001b[39m=\u001b[39mhow, has_dropped_na\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_dropped_na)\n\u001b[0;32m   1037\u001b[0m ids, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_info\n\u001b[0;32m   1038\u001b[0m ngroups \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngroups\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\_libs\\properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:942\u001b[0m, in \u001b[0;36mBaseGrouper.has_dropped_na\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    937\u001b[0m \u001b[39m@cache_readonly\u001b[39m\n\u001b[0;32m    938\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhas_dropped_na\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m    939\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[39m    Whether grouper has null value(s) that are dropped.\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_info[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39many())\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\_libs\\properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:946\u001b[0m, in \u001b[0;36mBaseGrouper.group_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[39m@cache_readonly\u001b[39m\n\u001b[0;32m    945\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgroup_info\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp], npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp], \u001b[39mint\u001b[39m]:\n\u001b[1;32m--> 946\u001b[0m     comp_ids, obs_group_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_compressed_codes()\n\u001b[0;32m    948\u001b[0m     ngroups \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(obs_group_ids)\n\u001b[0;32m    949\u001b[0m     comp_ids \u001b[39m=\u001b[39m ensure_platform_int(comp_ids)\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:972\u001b[0m, in \u001b[0;36mBaseGrouper._get_compressed_codes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    967\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_compressed_codes\u001b[39m(\n\u001b[0;32m    968\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    969\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39msignedinteger], npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp]]:\n\u001b[0;32m    970\u001b[0m     \u001b[39m# The first returned ndarray may have any signed integer dtype\u001b[39;00m\n\u001b[0;32m    971\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupings) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 972\u001b[0m         group_index \u001b[39m=\u001b[39m get_group_index(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcodes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, sort\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, xnull\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    973\u001b[0m         \u001b[39mreturn\u001b[39;00m compress_group_index(group_index, sort\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort)\n\u001b[0;32m    974\u001b[0m         \u001b[39m# FIXME: compress_group_index's second return value is int64, not intp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:897\u001b[0m, in \u001b[0;36mBaseGrouper.codes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    895\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    896\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcodes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39msignedinteger]]:\n\u001b[1;32m--> 897\u001b[0m     \u001b[39mreturn\u001b[39;00m [ping\u001b[39m.\u001b[39mcodes \u001b[39mfor\u001b[39;00m ping \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupings]\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:897\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    895\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    896\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcodes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39msignedinteger]]:\n\u001b[1;32m--> 897\u001b[0m     \u001b[39mreturn\u001b[39;00m [ping\u001b[39m.\u001b[39mcodes \u001b[39mfor\u001b[39;00m ping \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupings]\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:621\u001b[0m, in \u001b[0;36mGrouping.codes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_codes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     \u001b[39m# _codes is set in __init__ for MultiIndex cases\u001b[39;00m\n\u001b[0;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_codes\n\u001b[1;32m--> 621\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_codes_and_uniques[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\_libs\\properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:692\u001b[0m, in \u001b[0;36mGrouping._codes_and_uniques\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    685\u001b[0m     uniques \u001b[39m=\u001b[39m (\n\u001b[0;32m    686\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouping_vector\u001b[39m.\u001b[39mresult_index\u001b[39m.\u001b[39m_values  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     )\n\u001b[0;32m    688\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# GH35667, replace dropna=False with use_na_sentinel=False\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     \u001b[39m# error: Incompatible types in assignment (expression has type \"Union[\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[39m# ndarray[Any, Any], Index]\", variable has type \"Categorical\")\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     codes, uniques \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39mfactorize(  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouping_vector, sort\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort, use_na_sentinel\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dropna\n\u001b[0;32m    694\u001b[0m     )\n\u001b[0;32m    695\u001b[0m \u001b[39mreturn\u001b[39;00m codes, uniques\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\algorithms.py:822\u001b[0m, in \u001b[0;36mfactorize\u001b[1;34m(values, sort, na_sentinel, use_na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[39m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[0;32m    820\u001b[0m             values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[1;32m--> 822\u001b[0m     codes, uniques \u001b[39m=\u001b[39m factorize_array(\n\u001b[0;32m    823\u001b[0m         values,\n\u001b[0;32m    824\u001b[0m         na_sentinel\u001b[39m=\u001b[39mna_sentinel_arg,\n\u001b[0;32m    825\u001b[0m         size_hint\u001b[39m=\u001b[39msize_hint,\n\u001b[0;32m    826\u001b[0m     )\n\u001b[0;32m    828\u001b[0m \u001b[39mif\u001b[39;00m sort \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m na_sentinel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    830\u001b[0m         \u001b[39m# TODO: Can remove when na_sentinel=na_sentinel as in TODO above\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC-312\\.conda\\envs\\sg_i\\Lib\\site-packages\\pandas\\core\\algorithms.py:578\u001b[0m, in \u001b[0;36mfactorize_array\u001b[1;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[0;32m    575\u001b[0m hash_klass, values \u001b[39m=\u001b[39m _get_hashtable_algo(values)\n\u001b[0;32m    577\u001b[0m table \u001b[39m=\u001b[39m hash_klass(size_hint \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(values))\n\u001b[1;32m--> 578\u001b[0m uniques, codes \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39mfactorize(\n\u001b[0;32m    579\u001b[0m     values,\n\u001b[0;32m    580\u001b[0m     na_sentinel\u001b[39m=\u001b[39mna_sentinel,\n\u001b[0;32m    581\u001b[0m     na_value\u001b[39m=\u001b[39mna_value,\n\u001b[0;32m    582\u001b[0m     mask\u001b[39m=\u001b[39mmask,\n\u001b[0;32m    583\u001b[0m     ignore_na\u001b[39m=\u001b[39mignore_na,\n\u001b[0;32m    584\u001b[0m )\n\u001b[0;32m    586\u001b[0m \u001b[39m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[0;32m    587\u001b[0m uniques \u001b[39m=\u001b[39m _reconstruct_data(uniques, original\u001b[39m.\u001b[39mdtype, original)\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5943\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5857\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "WEIGHTS = [0.5, 0.5, 0]\n",
    "\n",
    "\n",
    "answers_grouped = answers.groupby(['OwnerUserId', 'MainTopic']).agg({'Score': 'sum', 'ViewCount': 'sum', 'Id': 'size'})\n",
    "comments_grouped = comments.groupby(['UserId', 'MainTopic']).agg({'Score': 'sum', 'Views': 'sum', 'Id': 'size'})\n",
    "posts_grouped = posts.groupby(['OwnerUserId', 'MainTopic']).agg({'Score': 'sum', 'Views': 'sum', 'Id': 'size'})\n",
    "\n",
    "def scoring(topic, weights, row):\n",
    "    # Retrieve data for the specific user and topic, if available\n",
    "    userAnswers = answers_grouped.loc[(row['Id'], topic)] if (row['Id'], topic) in answers_grouped.index else pd.Series({'Score': 0, 'ViewCount': 1, 'Id': 0})\n",
    "    userComments = comments_grouped.loc[(row['UserId'], topic)] if (row['UserId'], topic) in comments_grouped.index else pd.Series({'Score': 0, 'Views': 1, 'Id': 0})\n",
    "    userPosts = posts_grouped.loc[(row['Id'], topic)] if (row['Id'], topic) in posts_grouped.index else pd.Series({'Score': 0, 'Views': 1, 'Id': 0})\n",
    "\n",
    "    # Calculate the score\n",
    "    score = userAnswers['Id'] * (get_sentiment(row[\"AnswerSentiment\"], topic) * 20 * userAnswers['Score'] / userAnswers['ViewCount']) * weights[0] +\\\n",
    "            userComments['Id'] * max(userComments['Score'] / userComments['Views'], 1) * weights[1] +\\\n",
    "            max(userPosts['Score'] / userPosts['Views'], 1) * weights[2]\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USERS[\"Score\"] = USERS.apply(lambda row: [scoring(i, WEIGHTS, row) for i,_ in enumerate(TOPIC_TAGS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Convert the pandas DataFrame to a Dask DataFrame\n",
    "dask_users = dd.from_pandas(USERS, npartitions=8) # Adjust the number of partitions based on your dataset\n",
    "\n",
    "# Define a lambda function to be applied to each row\n",
    "lambda_function = lambda row: [scoring(i, WEIGHTS, row) for i, _ in enumerate(TOPIC_TAGS)]\n",
    "\n",
    "# Apply the function\n",
    "# Specify meta as a list, since the function returns a list\n",
    "dask_users['Score'] = dask_users.apply(lambda_function, axis=1, meta=('Score', 'object'))\n",
    "\n",
    "# Compute the results to get back a pandas DataFrame with a progress bar\n",
    "with ProgressBar():\n",
    "    USERS = dask_users.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sg_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
